- **Mastering Board Games by External and Internal Planning with Language Models**
  — [Paper](https://arxiv.org/pdf/2412.12119),
  [Tweet Leaed](https://x.com/ADarmouni/status/1874643013315518712)

This abouve one very interesting on how they translate into llm message?

Goes back into perhaps also in addition to building that observation space for the llm agent
observation space could be increased by the agent itself looking into using a tool, where he could pick
a number or sometthing from the stack in order to look back at history.. this is a bigger idea, the translation
function it self in the lead above will be helpful

- Agent runner?

- [Trelis Research, Maybe not pure RL this video.?
  I feel like this channel has a lot to learn from ]https://www.youtube.com/watch?v=bm6jegefGyY

- https://github.com/axon-rl/gem

What is an RL environment? w/ Nous Research's Roger Jin https://www.youtube.com/watch?v=zHaaivOQQGo&t
https://www.dropbox.com/scl/fi/2sqerxs9cp7fggqhznrdz/slides.pdf?rlkey=jfvigx4ljj3sws4jppeozvae5&e=2&st=e7wae9y6&dl=0

Will lead, https://x.com/willccbb/status/1940798666287337655, https://github.com/LeonGuertler/UnstableBaselines

https://pretty-radio-b75.notion.site/rLLM-A-Framework-for-Post-Training-Language-Agents-21b81902c146819db63cd98a54ba5f31 -> very good

https://leonguertler.github.io/2025/07/19/neurips-competition

https://github.com/WecoAI/aideml

Eriks Work

https://github.com/LeonGuertler/TextArena

https://x.com/dileeplearning/status/1701478267755384935

https://pbs.twimg.com/media/Grz81WAWQAAqvJd?format=jpg&name=4096x4096

https://github.com/collect-intel/llm-judge-bias-suite - her kan være veldig bra!

- Maybe some type of decorator of some sort to translate functions in openai ready function calling ready, validated cached or whatever? # For tools and decorating around functions, see docs on openai completaions?

- This type of trick could be used else where too?

- https://github.com/N8python/gpt-5-ppo-snake/tree/main

- A reward machine
  -> splits up 0 -> goal
- Diverse range of rewards

# the influence map, the observation space, the translation is the key, and what of the action space should/can be translated

- how do we build the observation, we got actions global niveau, local niveau, a root agent see and can peek into local niveau via looking at their interaction stacks
- a mechanism then to look at stack , observations at local niveau
- this is more of a mechanism than learning problem? but again it cal learn to use this tool.
- Translate games to LLM Speak
- Session step, agent step, translation step, game step

- hardmaru https://x.com/hardmaru/status/1534152930223239168 RTS inspo;)
  RTS

https://x.com/Dorialexander/status/1952753145206939728
