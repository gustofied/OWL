Great, I’ll look into best practices for unified logging in custom Python training code that uses logging, WandB, and Rerun—especially for visual outputs, future PyTorch Lightning or Hugging Face Trainer compatibility, and with an emphasis on developer ergonomics and experiment traceability.

I’ll focus on patterns for creating a central logging system using `dictConfig`, writing custom logging handlers, and clean integration of visuals from Rerun and WandB into the same pipeline.

I’ll get back to you shortly with practical recommendations, examples, and reusable code patterns.

# Best Practices for Logging ML Experiments with Python Logging, Rerun, and W\&B

## Overview of Logging Tools and Goals

In modern ML experiments, a robust logging setup is essential for both **developer ergonomics** and **experiment traceability**. In your case, you are using a combination of:

- **Standard Python logging** (for textual logs like info/debug messages and errors),
- **Rerun** (an open-source visual logging tool for images, sensor data, etc.), and
- **Weights & Biases (W\&B)** (a cloud-based experiment tracking tool for metrics, comparisons, and artifacts).

Each of these serves a different purpose. The goal is to integrate them so they complement each other without conflict. A good logging strategy will make it easy for you as a developer to monitor runs in real-time, while also preserving all important information for later analysis and reproducibility. Key best practices include using clear run identifiers, automating as much logging as possible, and capturing not just metrics but **all relevant outputs** – from hyperparameters to errors to visual artifacts. Below, we outline how to set up an effective multi-tool logging pipeline.

## Roles of Each Logging Tool

**Python Logging (text logs):** Use the built-in logging module to record textual information – e.g. status messages, warnings, errors, and custom events. These logs are crucial for debugging and auditing what happened during a run. You should configure Python logging to output to the console or a file, and also route these logs into Rerun's viewer for a unified experience (more on this below). By using logging levels (`DEBUG`, `INFO`, `ERROR`, etc.), you can control verbosity easily. For example, during development you might set it to `DEBUG` to see detailed messages, but in routine runs `INFO` or `WARNING` might suffice. This structured approach ensures important events (like exceptions or notable epoch events) are recorded and can be reviewed later for traceability.

**Rerun (visual and multimodal logs):** Rerun is a specialized SDK and viewer for **live visualization** of data (images, videos, point clouds, etc.) and time-series logs. It's excellent for **visual debugging** – as your model trains or your data pipeline runs, you can stream images (e.g. model predictions, camera inputs) to the Rerun viewer in real time. Rerun also supports textual logs in its timeline view. This means you can see your images side-by-side with log messages, correlated by time/step, which is very helpful for understanding context when something goes wrong. Rerun's open-source viewer runs locally and is optimized for high-throughput streaming; it even uses its own efficient `.rrd` file format for recording logs. You can run Rerun in **live mode** (streaming during the run) and optionally **record the session** to a file for later playback or sharing. Since Rerun is local (no internet needed) and built for speed, you can log lots of visual data (even every step if needed) without the network overhead – something that would be infeasible with exclusively cloud logging. In summary, use Rerun for rich **visual logs and real-time feedback**, and leverage its ability to save `.rrd` log files for post-mortem analysis or sharing of a run's entire timeline.

**Weights & Biases (metrics, artifacts, and experiment tracking):** W\&B excels at **experiment tracking and comparison** across runs. With a few lines of code you can initialize a run and log metrics, hyperparameters, and outputs to the W\&B cloud dashboard. This gives you persistent storage and a web UI to compare different runs' performance, which is crucial for traceability. W\&B will store each run's config (hyperparameters), system info, and metrics history by default, and you can also log **custom artifacts** like model checkpoints or evaluation results. It's great for **traceability** because every run gets a unique ID and you can add tags or names to identify it. Later, you or others can examine runs, see what hyperparams were used, view plotted training curves, and even **retrieve console logs** (W\&B can capture stdout/stderr by default and show them in the UI). You should use W\&B to log **metrics** (loss, accuracy, etc. over epochs/steps) and important **visuals or samples** (like a few example predictions) at certain intervals – not every single batch, but enough to document the model's progress. W\&B also lets you **compare runs side by side** easily and keeps a history of all experiments. In short, use W\&B for the **long-term record** of your experiments – ensuring reproducibility and easy sharing of results – while relying on Rerun and local logs for low-latency detailed insight during runtime.

## Unified Logging Setup

To get the **best of all worlds**, you can set up all three tools in a coordinated way. Here’s a recommended approach:

1. **Initialize W\&B at the start of the run:** Use `wandb.init(...)` at the beginning of your script or training loop to create a new run (with your project name, etc.). Save your hyperparameters into `wandb.config` (or directly via `wandb.init(config=...)`) so they are tracked. For example, log learning rates, model architecture, dataset name, seed, etc. This gives a baseline trace of what the run is. After that, W\&B will asynchronously log data you send to it.

2. **Initialize Rerun for visual logging:** Start Rerun at the beginning as well. For example, call `rr.init("run_name", spawn=True)` to initialize the Rerun SDK and spawn the viewer window. If you prefer to record to a file instead of live viewing (e.g., on a headless server), Rerun can operate in a headless recording mode (writing a `.rrd` file) which you can open later. Ensure Rerun is connected (either spawning a viewer or connecting to an existing one). This setup means any `rr.log()` calls will stream data to the viewer.

3. **Tie Python logging to both console/file and Rerun:** Configure the Python `logging` module so that it outputs to multiple targets. Typically, you'll want one handler for console (or a log file on disk) **and** one handler that sends logs to Rerun. Rerun provides a built-in `LoggingHandler` for Python logging integration. For example:

   ```python
   import logging, rerun as rr
   rr.init("my_run", spawn=True)
   logging.getLogger().setLevel(logging.DEBUG)  # or INFO as needed
   logging.getLogger().addHandler(rr.LoggingHandler("logs"))
   # Also add a console or file handler if not already configured:
   logging.basicConfig(level=logging.DEBUG)
   ```

   Using `rr.LoggingHandler("logs")` will forward all log records to Rerun (under the path "logs"), so you can see them in the Rerun viewer. The call above ensures any `logging.info("message")` or `logging.error("oops")` will appear in Rerun's **TextLog** view, time-synchronized with your other data. This is extremely useful for correlating events with visuals (for instance, seeing a spike in loss on W\&B and finding the corresponding image frame and log message in Rerun).

4. **Log metrics and other data during training:** As your training loop runs, use each tool appropriately:

   - Use `wandb.log({...})` to send scalar metrics to W\&B periodically (e.g. every epoch or every N batches). Log things like training loss, validation loss, accuracy, etc. If you have images or plots you want to persist, you can also use `wandb.log({"example_img": wandb.Image(np_array)})` occasionally – for instance, log a few sample predictions each epoch to W\&B. (Be mindful not to overload W\&B with too many images; a few per epoch is usually fine, whereas hundreds per second would be too much).
   - Use `rr.log()` for high-frequency visuals. For example, if you want to see how training is progressing every batch, you might log an image or a model output to Rerun each iteration. Rerun can handle streaming a large number of images efficiently for live debugging. These images won't all be stored long-term (unless you explicitly save the `.rrd`), but they let you observe behavior in detail (like a video). For any particularly important visuals, also log them to W\&B (which will store them) at key checkpoints – this satisfies traceability.
   - Continue to use `logging.info/debug` for any textual info. For example, you might `logging.info("Epoch %d completed", epoch)` or `logging.warning("Detected NaN in gradient!")`. Thanks to the setup, these messages will show in your console and in Rerun’s log view. Additionally, W\&B will capture console output automatically in many cases – if you run your script normally, W\&B's default settings capture stdout/stderr into the run's console logs panel in the UI. (This means your Python logs may also be viewable later on W\&B under **Run->Logs**, which is great for traceability).

5. **Log model checkpoints and artifacts:** When your model saves a checkpoint or when the run finishes, make sure to preserve those. W\&B provides an easy method to save artifacts – e.g., `wandb.log_artifact("model.ckpt")` to upload a model file. You should do this for experiment traceability: storing the final model (and possibly intermediate checkpoints or other outputs like a confusion matrix CSV, etc.) in W\&B. This way, every run not only has metrics but also the exact model weights or other artifacts needed to reproduce results. On the Rerun side, if you have been recording the session, you can finalize and save the `.rrd` log file (if not already saved). This `.rrd` file can be archived or even logged as a W\&B artifact if you want all logs in one place. Teams often share Rerun log files for offline review, so having it per run could be useful.

By following the above, you essentially have a **unified logging pipeline**: W\&B for configs/metrics/artifacts (the permanent record, accessible on the web), Rerun for live detailed insight (with an optional recorded log), and standard logging tying everything together with messages. This setup optimizes both **ergonomics** (you see what's happening as it happens, without having to dig through multiple disjoint tools) and **traceability** (everything important is saved with minimal manual steps).

## Logging More Than Just Metrics

A common mistake is to only log the primary metrics (like loss/accuracy), but for comprehensive experiment tracking you should log **all relevant information**. Based on your description, here are additional things to log and how to handle them:

- **Errors and Exceptions:** Always capture errors in your logs. Since you’re using Python’s logging, ensure that any caught exception is logged (e.g., use `logging.exception("Details")` in except blocks to log stack traces). W\&B will mark a run as failed if an uncaught exception occurs (when using `wandb.init` context manager, an exception automatically flags the run as failed). But even if the run crashes, having the error message in the logs (and in W\&B’s console log or Rerun’s log timeline) helps debug after the fact. This is crucial for traceability – you want to know _why_ a run failed. With the above setup, if something goes wrong, you can later open the W\&B run and inspect the console log, or open the Rerun `.rrd` and see the last messages and data before crash.
- **Events/Custom Markers:** Besides standard metrics, log important events. For instance, if you perform a certain data preprocessing step or a model checkpoint, log a message about it. You can even use special logging levels or markers for significant events (like `logging.info("Reached milestone: fine-tuning phase")`). These will appear in Rerun’s timeline and can act as bookmarks. If you're using W\&B, you can also use `wandb.log({"event": "description"}, step=...)` or W\&B **Mark Best** feature for checkpointing events, but a simple text log is often enough.
- **Model checkpoints:** As noted, treat model saving as a logging event. Log the fact that you saved a checkpoint (so the time of save is recorded in text logs), and actually save the file as an artifact. W\&B artifacts are perfect for this since they keep a versioned history of files (you might save `model_epoch5.pt`, `model_epoch10.pt`, etc., or just the final model). This ensures later you can retrieve any model associated with a given run. If not using W\&B artifacts, at least keep the files named with run IDs or timestamps. But since W\&B is already in use, it's wise to leverage it for storing checkpoints for complete experiment traceability.
- **User messages/interactions:** If your training involves any interactive input or you want to log configurations, you can log those as well. For example, if you manually trigger something or stop a run early, write that into the log ("Run stopped early by user at epoch 8"). This kind of note can be added as a W\&B **run note or tag** as well. Tagging the run (via `wandb.run.tags`) with things like `"early_stop"` or `"baseline_experiment"` can help you organize and recall context later.
- **Visualizations & Plots:** You mentioned **visuals a lot**, so presumably you generate images (like charts, sample outputs, etc.). Use both Rerun and W\&B here smartly. For dynamic visuals (e.g. a continually updating confusion matrix or an embedding projection that changes), Rerun can show these as they evolve. For W\&B, you can log static versions at intervals. W\&B allows logging custom matplotlib plots or charts with `wandb.log` as images. Also, W\&B Tables or Plotly integrations could be useful if you want to log, say, an interactive chart of something at the end. The key is: **don’t lose this valuable info** – include representative visuals in your experiment log. They often reveal things metrics alone won't (for example, the quality of generated images can be tracked via logged samples).
- **System/resource metrics:** While not asked explicitly, for traceability you might consider logging system metrics (like GPU utilization, memory). W\&B can automatically track some of this (if you enable system monitoring or if using Hugging Face Trainer which logs CPU/GPU usage automatically). It’s not a top priority, but if experiments sometimes stall or slow down, having resource logs can help pinpoint why. W\&B’s "system metrics" panel can record CPU, GPU, RAM usage if configured. If not using that, you could log a few snapshots via Python (e.g., using `psutil`) into W\&B or print them to logs.

In summary, **think of logging in categories**: hyperparameters/config (log at start), metrics (log throughout), **visual outputs** (log to Rerun continuously, to W\&B periodically), **text logs** for events and errors (continuous), and **artifacts** like models (at end or checkpoints). By covering all these, you ensure end-to-end traceability – anyone can later take a run and see what data went in, what settings were used, how it progressed, what issues occurred, and what the outcomes were.

## Framework Integration (PyTorch Lightning, Hugging Face Trainer)

You mentioned possibly using PyTorch Lightning or Hugging Face’s Trainer in the future. Both frameworks can simplify logging because they have integration with W\&B (and others) out-of-the-box, and they manage some logging boilerplate for you:

- **PyTorch Lightning (PL):** Lightning has a built-in W\&B logger (`WandbLogger`) and supports attaching multiple loggers simultaneously. If you move to Lightning, you could instantiate a WandbLogger (which under the hood calls `wandb.init`) and pass it to the `Trainer`. You can also add a CSVLogger or others at the same time. Lightning will automatically log metrics that you report with `self.log()` inside your `LightningModule` – these will go to W\&B without you directly calling `wandb.log`. This improves developer ergonomics because you just focus on `self.log("metric_name", value)` in your training/validation step, and Lightning handles the rest (with proper step, epoch, etc.). Lightning also captures stdout and sends it to its log files by default (you can configure that), and you can still attach the Rerun logging handler to Python logging if you want to stream those logs to Rerun's viewer. Since Lightning allows multiple loggers, one approach is: use WandbLogger for experiment tracking, and create a **custom logger** for Rerun (Lightning doesn’t have a RerunLogger built-in, but you could write a small subclass of the Lightning `Logger` interface that calls Rerun API). Alternatively, simpler: use a Lightning **Callback** to call `rr.log` at certain intervals for visuals. For instance, a callback’s `on_batch_end` could send data to Rerun. This way, even with Lightning’s abstraction, you keep the rich visual logging. The bottom line is Lightning can be configured to work with your existing tools, and it will help with organizing logs (for example, it automatically saves a local log file of training progress and can checkpoint models).
- **Hugging Face Trainer:** The `transformers.Trainer` integrates with W\&B very seamlessly – if `wandb` is installed and you call `trainer.train()`, it will auto-initialize a W\&B run and log training/validation metrics, learning rates, etc., without extra code. This is very convenient (just set `report_to="wandb"` in TrainingArguments or let it auto-detect). It logs metrics at each `logging_steps` interval and at the end of epochs. For custom logging (like additional metrics or images), you can either subclass Trainer or use callbacks. Hugging Face provides a `WandbCallback` for extra logging (e.g., logging model predictions every epoch to W\&B). To use Rerun with HF Trainer, you’d similarly use a callback approach: e.g., a `TrainerCallback` where in `on_step_end` or `on_log` you call `rr.log` with whatever visuals you want. HF Trainer doesn’t natively know about Rerun, but you can integrate it manually. The Trainer will also print logs to stdout (which W\&B captures and you can send to Rerun via the logging handler as before). So moving to HF Trainer can automate metric logging and still allows manual hooks for other logging.

**Distributed settings (future consideration):** You noted you are not running distributed training right now. When/if you do (multiple GPUs or nodes), be mindful of logging in multi-process scenarios:

- For W\&B: typically you only want one process (usually rank 0) to log metrics to avoid duplicate entries. Both Lightning and HF Trainer handle this for you (they only log from rank 0 by default). In custom code, you’d add a check like `if dist.get_rank() == 0: wandb.log(...)`.
- For Python logging: if you have multiple processes all writing to the same log file or console, it can get messy. Often, one design is to have only the main process handle logging or use separate log files per process. Rerun actually can accept data from multiple processes if they connect to the same Rerun server (since it’s built for robotics scenarios), but you have to manage unique stream IDs to not collide. If in the future you go distributed, you might run one Rerun per process on different ports, or only have one process do the Rerun logging of aggregated data. This is a complex topic, but the **good news** is that with frameworks like Lightning/HF, they abstract a lot of this: e.g., Lightning’s `self.log(..., sync_dist=True)` can accumulate metrics across GPUs and still only log once. So for now, in single-process mode, you are fine – just keep in mind to structure code such that adding a rank check is easy if needed later.

## Developer Ergonomics and Experiment Traceability Tips

Finally, here are some overarching best practices focusing on making logging convenient and ensuring experiments are traceable:

- **Automate and integrate logging into training scripts**: As highlighted in general ML ops advice, you should _not_ rely on manual steps for logging. The setup we described can be made into a reusable initialization function. For example, write a function `setup_logging(run_name, use_rerun=True, use_wandb=True)` that performs all the initialization (W\&B init, Rerun init, logging config). This way, every experiment run calls this and you never forget to start a logger or mis-configure it. It reduces human error and makes the process easy, improving your productivity.
- **Use unique run names or IDs**: Ensure each run is uniquely identifiable. W\&B does this automatically (with run IDs and incremental names if you don’t name them), but it's good to name runs descriptively (e.g. "resnet50_experiment1_lr1e-3"). This name can propagate – you can use it as part of the Rerun session name, and in filenames for local logs or model files. Unique naming avoids confusion and helps when searching for a run later. In W\&B you can also add **tags** (like "baseline", "experiment_A") which is helpful for grouping. Traceability is enhanced when you can quickly map a result back to the exact run that produced it.
- **Store metadata and configuration**: Always log the metadata of your experiments. This includes hyperparameters (learning rate, batch size, etc.), but also things like dataset version or split, random seed, hardware used, and code version. W\&B's `wandb.config` is great for this – you can `.update()` it with a dict of all your settings at runtime, and they’ll be saved. Additionally, W\&B by default tries to capture the git commit if your repo is clean. Make sure this is working – it greatly aids reproducibility if you know which code version a run used. If not using git or wanting extra safety, log the code diff or copy of the training script (W\&B has a code saving feature, or you can attach files as artifacts). Consider also logging any noteworthy environment info (like `torch.__version__`, `cuda version`). These details ensure that months later, you or someone else can retrace the experiment environment exactly.
- **Leverage visualization and comparison tools**: Since you have W\&B, use its UI to compare runs (learning curves, etc.) and annotate insights. W\&B allows grouping runs into an "experiment" for comparison – e.g., if you do five runs trying different hyperparameters, give them a common tag or group name. This, combined with the visualizations of metrics, helps you quickly derive conclusions. For Rerun, while it’s more for single-run visualization, you can still compare qualitatively (e.g., open two Rerun files side by side if needed). The point is to **utilize these tools to the fullest**: they are not just dumping grounds for logs, but active dashboards for understanding your model training. Given that traceability is a priority, you might even create a brief W\&B **Report** after an experiment, embedding plots and images from the run – this solidifies the knowledge and is good practice in research/industry settings.
- **Manage logging frequency and volume**: For developer convenience, log as much as is useful, but be mindful of overlogging. For instance, writing huge amounts of data can slow down training or clutter dashboards. W\&B has some soft limits and performance guidelines (e.g., on the order of tens of thousands of points or images per run is OK, but millions is not). Rerun can handle high frequency data better locally, but it too can become unwieldy if you log enormous data every step (e.g., very high-res images at 100 FPS might strain memory). The best practice is to **log at a meaningful frequency**: maybe every batch to Rerun (if needed for smooth visualization), but to W\&B only every Nth batch or each epoch for metrics. Similarly, log a moderate number of images (say 5-10 images per epoch to W\&B, rather than every single image). This keeps the experiment logs informative but manageable. You already indicated no heavy distributed setup now, but if you later run longer experiments, you might also use W\&B’s **offline/online mode** and sync later if needed (for when internet is an issue) – just a note.
- **Test your logging setup**: When you implement this multi-logger approach, do a dry run with a short training job to ensure everything works together. Check that: logs appear in Rerun viewer, metrics show up on W\&B (with correct step indices), no major performance hit is observed (e.g., if W\&B logging is synchronous by default, you can make it asynchronous by using `wandb.init(settings=wandb.Settings(_disable_stats=True))` or similar tweaks if needed). Also verify that when an experiment is finished, you can retrieve all the pieces (open W\&B and see metrics, open Rerun file if saved, see log file, etc.). A bit of upfront validation will give you confidence that during real runs you won't miss critical data.

By adhering to these practices, you'll achieve a smooth and powerful logging system: during training you have a **developer-friendly view** (lots of insight, low effort), and after training you have a **traceable record** (everything needed to understand and reproduce the experiment). Logging might seem verbose, but it pays off when you can pinpoint exactly why model version A outperformed B or why a certain run crashed. In essence, you're creating an **experiment diary** that future you (or colleagues) will thank you for.

## Conclusion

Bringing it all together, the "best way" to do logging in your scenario is to **use each tool for its strengths and ensure they work in concert**. Use W\&B for robust experiment tracking – configs, metrics, comparisons, and artifacts storage. Simultaneously, use Rerun for rich real-time visual logging and debugging, with the option to save those logs for later analysis. And underpin it with Python’s logging to capture text outputs and errors, feeding those into both your console and Rerun for context. This multi-faceted approach covers all types of logs: from simple numbers to complex images. It emphasizes developer ergonomics (you get immediate feedback and don’t have to do tedious manual logging) and experiment traceability (everything important is recorded and organized).

By planning for the future (unique run IDs, considering framework integrations, scaling to more complex setups) and following established best practices, you will set yourself up for success as your experiments grow. Each run you do will be reproducible and analyzable, and adding new logging outputs (say, another tool or more data types) will be as simple as plugging into your existing pipeline. Logging may not be the flashiest part of machine learning, but when done right, it becomes the backbone of a rigorous and efficient experimentation process. Good luck, and happy logging!

**Sources:**

- Weights & Biases documentation – logging experiments, metrics, and artifacts
- Rerun documentation – integrating with Python logging and visual data streaming
- Best-practice recommendations for experiment tracking and logging in ML (e.g., unique naming, automated logging, metadata)
- PyTorch Lightning docs – support for multiple loggers and W\&B integration
- Discussion of Rerun’s log file format and usage for sharing visual logs
- Hugging Face forums – note on Trainer’s automatic W\&B logging of metrics/resources
